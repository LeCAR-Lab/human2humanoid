seed : 1
runner_class_name : 'OnPolicyRunner'
policy:
    init_noise_std : 1.0
    actor_hidden_dims : [512, 256, 128]
    critic_hidden_dims : [512, 256, 128]
    activation : 'elu' # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
    # only for 'ActorCriticRecurrent':
    # rnn_type : 'lstm'
    # rnn_hidden_size : 512
    # rnn_num_layers : 1

add_short_history: False
short_history_length: 5

algorithm:
    # training params
    value_loss_coef : 1.0
    use_clipped_value_loss : True
    clip_param : 0.2
    entropy_coef : 0.01
    num_learning_epochs : 5
    num_mini_batches : 4 # mini batch size : num_envs*nsteps / nminibatches
    learning_rate : 1.e-3 #5.e-4
    schedule : 'adaptive' # could be adaptive, fixed
    gamma : 0.99
    lam : 0.95
    desired_kl : 0.01
    max_grad_norm : 1.
    action_smoothness_coef : 0.0

runner:
    policy_class_name : 'ActorCritic'
    # policy_calss_name : 'ActorCriticRecurrent'
    algorithm_class_name : 'PPO'
    num_steps_per_env : 24 # per iteration
    max_iterations : 100000 # number of policy updates

    # logging
    save_interval : 500 # check for potential saves every this many iterations
    experiment_name : 'test'
    run_name : ''
    # load and resume
    resume : False
    load_run : -1 # -1 : last run
    checkpoint : -1 # -1 : last saved model
    resume_path : None # updated from load_run and chkpt
    
